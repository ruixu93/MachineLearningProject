{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  Preprocesscopy import Preprocesscopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = '/home/rui/Documents/MLProject/data/modified-crab/trips-in-sanfran.csv'\n",
    "city = 'san_francisco'\n",
    "tile_size = 0.360\n",
    "la_size = 0.003234\n",
    "long_size = 0.004049\n",
    "start_id = 0\n",
    "freq_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Preprocesscopy(file_path, city, tile_size, la_size, long_size, freq_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.read_dataset()\n",
    "print \"done read dataset\"\n",
    "data.split_train_test()\n",
    "print \"done split dataset\"\n",
    "data.find_significant_region(start_id)\n",
    "print \"done find significant region\"\n",
    "data.filter_low_frequency_trajectory()\n",
    "print \"done filter low frequency traj\"\n",
    "data.refind_significant_region(start_id)\n",
    "print \"done refind significant region\"\n",
    "data.convert_train_traj_to_region_id()\n",
    "print \"done convert train traj to region id\"\n",
    "data.convert_test_traj_to_region_id()\n",
    "print \"done convert test traj to region id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = data.taxi_train\n",
    "test_data = data.taxi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data.region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 3: get trip trace and pad.\n",
    "context_window = 2\n",
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for row_index, row in train_data.iterrows():\n",
    "    single_trip = row[3]\n",
    "    trip_length = len(single_trip)\n",
    "\n",
    "    for index in xrange(trip_length):\n",
    "        if index <= context_window-1:\n",
    "            continue\n",
    "        train_X.append(single_trip[index - context_window:index])\n",
    "        train_y.append(single_trip[index])\n",
    "\n",
    "    if row_index % 10000 == 0:\n",
    "        print row_index\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "print train_X.shape\n",
    "\n",
    "print 'finish extract data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 3: get trip trace and pad.\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for row_index, row in test_data.iterrows():\n",
    "    single_trip = row[3]\n",
    "    trip_length = len(single_trip)\n",
    "\n",
    "    for index in xrange(trip_length):\n",
    "        if index <= context_window-1:\n",
    "            continue\n",
    "        test_X.append(single_trip[index - context_window:index])\n",
    "        test_y.append(single_trip[index])\n",
    "\n",
    "    if row_index % 10000 == 0:\n",
    "        print row_index\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "print test_X.shape\n",
    "\n",
    "print 'finish extract data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 4: build and train a simple feedforward neural network model\n",
    "batch_size = 32\n",
    "vocabulary_size = len(data.region_dict)\n",
    "embedding_size = 50\n",
    "hidden_size = 75\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator():\n",
    "    \"\"\" A simple data iterator \"\"\"\n",
    "    data_len = len(train_X)\n",
    "\n",
    "    idxs = np.arange(0, data_len)\n",
    "    np.random.shuffle(idxs)\n",
    "    shuf_features = train_X[idxs]\n",
    "    shuf_labels = train_y[idxs]\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_len):\n",
    "            batch_train_X = shuf_features[batch_size*i: batch_size*(i+1)]\n",
    "            batch_train_y = shuf_labels[batch_size*i: batch_size*(i+1)]\n",
    "            yield batch_train_X, batch_train_y\n",
    "\n",
    "\n",
    "iter_ = data_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_iterator():\n",
    "    \"\"\" A simple data iterator \"\"\"\n",
    "    data_len = len(test_X)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_len):\n",
    "            batch_test_X = test_X[batch_size*i: batch_size*(i+1)]\n",
    "            batch_test_y = test_y[batch_size*i: batch_size*(i+1)]\n",
    "            yield batch_test_X, batch_test_y\n",
    "\n",
    "\n",
    "test_iter_ = test_data_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6: define the model and train\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    inputs_data = tf.placeholder(tf.int32, [batch_size, context_window])\n",
    "    outputs_data = tf.placeholder(tf.int32, [batch_size])\n",
    "\n",
    "    projection_weight = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size]))\n",
    "\n",
    "    hidden_weight_1 = tf.Variable(tf.random_uniform([embedding_size, hidden_size]))\n",
    "    hidden_bias_1 = tf.Variable(tf.random_uniform([hidden_size]))\n",
    "    \n",
    "    hidden_weight_2 = tf.Variable(tf.random_uniform([embedding_size, hidden_size]))\n",
    "    hidden_bias_2 = tf.Variable(tf.random_uniform([hidden_size]))\n",
    "\n",
    "    output_weight = tf.Variable(tf.random_uniform([2*hidden_size, embedding_size]))\n",
    "    output_bias = tf.Variable(tf.random_uniform([embedding_size]))\n",
    "    \n",
    "    softmax_weight = tf.Variable(tf.random_uniform([embedding_size, vocabulary_size]))\n",
    "\n",
    "    projection_layer_1 = tf.nn.embedding_lookup(projection_weight, inputs_data[:, 0])\n",
    "    hidden_layer_1 = tf.nn.relu(tf.matmul(projection_layer_1, hidden_weight_1) + hidden_bias_1)\n",
    "    \n",
    "    projection_layer_2 = tf.nn.embedding_lookup(projection_weight, inputs_data[:, 1])\n",
    "    hidden_layer_2 = tf.nn.relu(tf.matmul(projection_layer_2, hidden_weight_2) + hidden_bias_2)\n",
    "\n",
    "    hidden_layer = tf.concat(1, [hidden_layer_1, hidden_layer_2])\n",
    "\n",
    "    output_layer = tf.nn.relu(tf.matmul(hidden_layer, output_weight) + output_bias)\n",
    "    \n",
    "    logits = tf.matmul(output_layer, softmax_weight)\n",
    "    \n",
    "    #logits = tf.matmul(output_layer, tf.transpose(projection_weight))\n",
    "\n",
    "    true_location = outputs_data\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=true_location)\n",
    "\n",
    "    mean_loss = tf.reduce_mean(cross_entropy)\n",
    "    total_loss = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "    prediction_accuracy = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(true_location, tf.int64)), tf.float32)\n",
    "    mean_accuracy = tf.reduce_mean(prediction_accuracy)\n",
    "    total_correct = tf.reduce_sum(prediction_accuracy)\n",
    "\n",
    "    # training step\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_loss)\n",
    "\n",
    "    # add variable initializer\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # step 6: begin training\n",
    "    sess.run(init)\n",
    "    num_epoch = 1000\n",
    "    batch_len = len(train_X) // batch_size\n",
    "    print \"begin training the rnn\"\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        for _ in range(batch_len):\n",
    "\n",
    "            # get a batch of data\n",
    "            x_batch, y_batch = iter_.next()\n",
    "\n",
    "            # pass it in as through feed_dict\n",
    "            _, loss_val, accuracy_val = sess.run([train_step, mean_loss, mean_accuracy], feed_dict={\n",
    "                            inputs_data: x_batch,\n",
    "                            outputs_data: y_batch\n",
    "                            })\n",
    "\n",
    "        training_total_num_prediction = 0\n",
    "        training_total_loss_val = 0\n",
    "        training_total_num_correct = 0\n",
    "        for _ in range(batch_len):\n",
    "            # get a batch of data\n",
    "            x_batch, y_batch = iter_.next()\n",
    "\n",
    "            num_pred = x_batch.shape[0]\n",
    "\n",
    "            # pass it in as through feed_dict\n",
    "            loss_val, num_correct = sess.run([total_loss, total_correct], feed_dict={\n",
    "                            inputs_data: x_batch,\n",
    "                            outputs_data: y_batch\n",
    "                            })\n",
    "\n",
    "            training_total_num_prediction += num_pred\n",
    "            training_total_loss_val += loss_val\n",
    "            training_total_num_correct += num_correct\n",
    "        training_overall_mean_loss = training_total_loss_val / training_total_num_prediction\n",
    "        training_overall_accuracy = training_total_num_correct / training_total_num_prediction\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            test_total_num_prediction = 0\n",
    "            test_total_loss_val = 0\n",
    "            test_total_num_correct = 0\n",
    "            for _ in range(batch_len):\n",
    "                # get a batch of data\n",
    "                x_batch, y_batch = test_iter_.next()\n",
    "\n",
    "                num_pred = x_batch.shape[0]\n",
    "\n",
    "                # pass it in as through feed_dict\n",
    "                loss_val, num_correct = sess.run([total_loss, total_correct], feed_dict={\n",
    "                                inputs_data: x_batch,\n",
    "                                outputs_data: y_batch\n",
    "                                })\n",
    "\n",
    "                test_total_num_prediction += num_pred\n",
    "                test_total_loss_val += loss_val\n",
    "                test_total_num_correct += num_correct\n",
    "            test_overall_mean_loss = test_total_loss_val / test_total_num_prediction\n",
    "            test_overall_accuracy = test_total_num_correct / test_total_num_prediction\n",
    "            print \"new test accuracy\"\n",
    "            \n",
    "        print \"Epoch %d The loss is %f, the training accuracy is %f, the test accuracy is %f\" % (epoch, training_overall_mean_loss, \n",
    "                                                                                                 training_overall_accuracy, test_overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
